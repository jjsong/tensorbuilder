<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>tensorbuilder.builders API documentation</title>
    <meta name="description" content="# Tensor Builder

TensorBuilder is light wrapper over TensorFlow that enables you to easily create c..." />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">

    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#tensorbuilder.builders.branches">branches</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.builder">builder</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.builders.Builder">Builder</a></span>
        
          
  <ul>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.__init__">__init__</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.branch">branch</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.connect_bias">connect_bias</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.connect_layer">connect_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.connect_weights">connect_weights</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.copy">copy</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.map">map</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.Builder.then">then</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.builders.BuilderTree">BuilderTree</a></span>
        
          
  <ul>
    <li class="mono"><a href="#tensorbuilder.builders.BuilderTree.__init__">__init__</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.BuilderTree.builders">builders</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.BuilderTree.connect_layer">connect_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.BuilderTree.copy">copy</a></li>
    <li class="mono"><a href="#tensorbuilder.builders.BuilderTree.tensors">tensors</a></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">tensorbuilder.builders</span> module</h1>
  <h1>Tensor Builder</h1>
<p>TensorBuilder is light wrapper over TensorFlow that enables you to easily create complex deep neural networks using the Builder Pattern through a functional <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent</a> <a href="https://en.wikipedia.org/wiki/Immutable_object">immutable</a> API. The main goals of TensorBuilder are</p>
<ul>
<li>Be a light-weight wrapper around TensorFlow fully compatible with its functions.</li>
<li>Let the user name and inspect the tensorflow Variables generated by TensorBuilder</li>
<li>Enable the user to create complex branched topologies while maintaining a fluent API (see <a href="http://cgarciae.github.io/tensorbuilder/tensorbuilder.m.html#tensorbuilder.builders.Builder.branch">Builder.branch</a>)</li>
</ul>
<p>TensorBuilder has a small set of primitives that enable you to express complex networks while maintaining a consistent API. Its branching mechanism enables you to express through the structure of your code the structure of the network, even when you have complex sub-branching expansions and reductions, all this while keeping the same fluid API.</p>
<p>TensorBuilder takes inspiration from <a href="https://github.com/google/prettytensor">prettytensor</a> but its internals are simpler, its API is smaller but equally powerfull, its branching mechanism is more expresive and doesn't break the fluent API, and its immutable nature helps avoid a lot of conceptual complexity.</p>
<h2>Installation</h2>
<p>Tensor Builder assumes you have a working <code>tensorflow</code> installation. We don't include it in the <code>requirements.txt</code> since the installation of tensorflow varies depending on your setup.</p>
<h4>From source</h4>
<ol>
<li><code>git clone https://github.com/cgarciae/tensorbuilder.git</code></li>
<li><code>cd tensorbuilder</code></li>
<li><code>python setup.py install</code></li>
</ol>
<h4>From pip</h4>
<p>Coming soon!</p>
<h2>Getting Started</h2>
<p>Create neural network with a [5, 10, 3] architecture with a <code>softmax</code> output layer and a <code>tanh</code> hidden layer through a Builder and then get back its tensor:</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span>
<span class="p">)</span>
</pre></div>


<h2>Branching</h2>
<p>If you are sufficiently familiar with tensorflow or use prettytensor then you might appreciate the branching capabilities of Tensor Builder in this (overly complex) example</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">root</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">root</span>
        <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">root</span>
        <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
        <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">root2</span><span class="p">:</span>
        <span class="p">[</span>
          <span class="n">root2</span>
          <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
        <span class="p">,</span>
          <span class="n">root2</span>
          <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
          <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
        <span class="p">])</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
    <span class="o">.</span><span class="n">tensor</span>
<span class="p">)</span>
</pre></div>


<h2>Documentation</h2>
<p>The main documentaion is in the <a href="http://cgarciae.github.io/tensorbuilder/tensorbuilder.m.html">tensorbuilder module</a>. The documentation for the complete project is <a href="http://cgarciae.github.io/tensorbuilder/">here</a>.</p>
<h2>Examples</h2>
<p>Here are the examples for each method of the API. If you are understand all examples, then you've understood the complete API.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># Tensor Builder</span>

<span class="sd">TensorBuilder is light wrapper over TensorFlow that enables you to easily create complex deep neural networks using the Builder Pattern through a functional [fluent](https://en.wikipedia.org/wiki/Fluent_interface) [immutable](https://en.wikipedia.org/wiki/Immutable_object) API. The main goals of TensorBuilder are</span>

<span class="sd">* Be a light-weight wrapper around TensorFlow fully compatible with its functions.</span>
<span class="sd">* Let the user name and inspect the tensorflow Variables generated by TensorBuilder</span>
<span class="sd">* Enable the user to create complex branched topologies while maintaining a fluent API (see [Builder.branch](http://cgarciae.github.io/tensorbuilder/tensorbuilder.m.html#tensorbuilder.builders.Builder.branch))</span>

<span class="sd">TensorBuilder has a small set of primitives that enable you to express complex networks while maintaining a consistent API. Its branching mechanism enables you to express through the structure of your code the structure of the network, even when you have complex sub-branching expansions and reductions, all this while keeping the same fluid API.</span>

<span class="sd">TensorBuilder takes inspiration from [prettytensor](https://github.com/google/prettytensor) but its internals are simpler, its API is smaller but equally powerfull, its branching mechanism is more expresive and doesn&#39;t break the fluent API, and its immutable nature helps avoid a lot of conceptual complexity.</span>

<span class="sd">## Installation</span>
<span class="sd">Tensor Builder assumes you have a working `tensorflow` installation. We don&#39;t include it in the `requirements.txt` since the installation of tensorflow varies depending on your setup.</span>

<span class="sd">#### From source</span>
<span class="sd">1. `git clone https://github.com/cgarciae/tensorbuilder.git`</span>
<span class="sd">2. `cd tensorbuilder`</span>
<span class="sd">3. `python setup.py install`</span>

<span class="sd">#### From pip</span>
<span class="sd">Coming soon!</span>

<span class="sd">## Getting Started</span>

<span class="sd">Create neural network with a [5, 10, 3] architecture with a `softmax` output layer and a `tanh` hidden layer through a Builder and then get back its tensor:</span>

<span class="sd">    import tensorflow as tf</span>
<span class="sd">    import tensorbuilder as tb</span>

<span class="sd">    x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">    h = (</span>
<span class="sd">        x.builder()</span>
<span class="sd">        .connect_layer(10, fn=tf.nn.tanh)</span>
<span class="sd">        .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">        .tensor</span>
<span class="sd">    )</span>

<span class="sd">## Branching</span>
<span class="sd">If you are sufficiently familiar with tensorflow or use prettytensor then you might appreciate the branching capabilities of Tensor Builder in this (overly complex) example</span>

<span class="sd">    import tensorflow as tf</span>
<span class="sd">    import tensorbuilder as tb</span>

<span class="sd">    x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">    keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">    h = (</span>
<span class="sd">        x.builder()</span>
<span class="sd">        .connect_layer(10)</span>
<span class="sd">        .branch(lambda root:</span>
<span class="sd">        [</span>
<span class="sd">            root</span>
<span class="sd">            .connect_layer(3, fn=tf.nn.relu)</span>
<span class="sd">        ,</span>
<span class="sd">            root</span>
<span class="sd">            .connect_layer(9, fn=tf.nn.tanh)</span>
<span class="sd">            .branch(lambda root2:</span>
<span class="sd">            [</span>
<span class="sd">              root2</span>
<span class="sd">              .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">            ,</span>
<span class="sd">              root2</span>
<span class="sd">              .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">              .connect_layer(8, tf.nn.softmax)</span>
<span class="sd">            ])</span>
<span class="sd">        ])</span>
<span class="sd">        .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">        .tensor</span>
<span class="sd">    )</span>

<span class="sd">## Documentation</span>

<span class="sd">The main documentaion is in the [tensorbuilder module](http://cgarciae.github.io/tensorbuilder/tensorbuilder.m.html). The documentation for the complete project is [here](http://cgarciae.github.io/tensorbuilder/).</span>

<span class="sd">## Examples</span>

<span class="sd">Here are the examples for each method of the API. If you are understand all examples, then you&#39;ve understood the complete API.</span>


<span class="sd">&quot;&quot;&quot;</span>

<span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.0.1&quot;</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">decorator</span> <span class="kn">import</span> <span class="n">decorator</span>


<span class="c1"># Decorators</span>
<span class="nd">@decorator</span>
<span class="k">def</span> <span class="nf">_immutable</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decorator. Passes a copy of the entity to the method so that the original object remains un touched.</span>
<span class="sd">    Used in methods to get a fluent immatable API.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>



<span class="k">class</span> <span class="nc">Builder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Builder class is a wrapper around a Tensor. Most of its method are immutable in the sense that they don&#39;t modify the caller object but rather always make a copy, they also tend to return a Builder so you you can keep fluently chaining methods.</span>

<span class="sd">    To create a builder from a method you have these options:</span>

<span class="sd">    1. Use the `tensorbuilder.builders.builder` function</span>

<span class="sd">            tb.builder(tensor)</span>

<span class="sd">    2. Use the monkey-patched method on the Tensor class</span>

<span class="sd">            tensor.builder()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">{}):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="sd">&quot;&quot;&quot;A `tensorflow` Tensor.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variables</span> <span class="o">=</span> <span class="n">variables</span>
        <span class="sd">&quot;&quot;&quot;A dictionary that accumulates the **tf.Variable** tensors generated during the building process, it has the form {tensor_name: String -&gt; tensor: tf.Variable}. Since functions like `tensorbuilder.builders.Builder.connect_layer` hides you the complexity of creating the **bias** and **weights** of your network, `tensorbuilder.builders.Builder` stores them in this field. The methods of this class enables you to set the names of these variables, but take into account that the final name is actually the name of the tensor, with is set by `tensorflow`. Check their documentation to see how the name is defined.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a copy of this Builder&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_weights</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **w** be a **tf.Variable** of shape **[n, size]**. Then `builder.connect_weights(size)` computes `tf.matmul(x, w)`.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">        * `name`: the name of the tensor (default: &quot;connect_weights&quot;)</span>
<span class="sd">        * `weights_name`: the name of the **w** tensor of type `tf.Variable`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds `tf.matmul(x, w)`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            z = x.builder().connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">size</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">weights_name</span> <span class="k">if</span> <span class="n">weights_name</span> <span class="k">else</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span>

        <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_bias</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **b** be a **tf.Variable** of shape **[n]**. Then `builder.connect_bias()` computes `tf.add(x, b)`.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **b** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `name`: the name of the tensor (default: &quot;connect_bias&quot;)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: &quot;b&quot;).</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds `tf.matmul(x, w) + b`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            z = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">                .connect_bias(bias_name=&quot;bias&quot;)</span>
<span class="sd">            )</span>

<span class="sd">        Note, the previous is equivalent to using `tensorbuilder.builders.Builder.connect_layer` like this</span>

<span class="sd">            z = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, weights_name=&quot;weights&quot;, bias_name=&quot;bias&quot;)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">bias_name</span> <span class="k">if</span> <span class="n">bias_name</span> <span class="k">else</span> <span class="n">b</span><span class="o">.</span><span class="n">name</span>

        <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>


    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, let **w** be a **tf.Variable** of shape **[n, size]**, let **b** be a **tf.Variable** of shape **[n]**, and **fn** be a function from a tensor to a tensor. Then `builder.connect_layer(size, fn=fn)` computes `fn(tf.matmul(x, w) + b)`. If **fn** is not present the layer is linear.</span>

<span class="sd">        Note that **fn** must expose the keyword/named argument `name`, this is compatible with the tensorflow API.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **b** and **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">        * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">        * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">        * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">        * `weights_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        The previous is equivalent to using</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_weights(3)</span>
<span class="sd">                .connect_bias()</span>
<span class="sd">                .map(tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        You can chain various `connect_layer`s to get deeper neural networks</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">                .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>


        <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
            <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` and **fn** be a function from a tensor to a tensor. Then `builder.map(fn)` computes `fn(x)`. All extra positional and named arguments are forwarded to **fn** such that</span>

<span class="sd">            builder.map(fn, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>

<span class="sd">        internally results in</span>

<span class="sd">            builder.tensor = fn(builder.tensor, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following constructs a neural network with the architecture `[40 input, 100 tanh, 30 softmax]` and and applies `dropout` to the tanh layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">                .map(tb.nn.dropout, keep_prob)</span>
<span class="sd">                .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">then</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Expects a function **fn** with type `builder -&gt; builder`. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `builder -&gt; builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        The following *manually* constructs the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)` while updating the `tensorbuilder.tensorbuiler.Builder.variables` dictionary.</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            def sigmoid_layer(builder, size):</span>
<span class="sd">                m = int(builder.tensor.get_shape()[1])</span>
<span class="sd">                n = size</span>

<span class="sd">                w = tf.Variable(tf.random_uniform([m, n], -1.0, 1.0))</span>
<span class="sd">                b = tf.Variable(tf.random_uniform([n], -1.0, 1.0))</span>

<span class="sd">                builder.variables[w.name] = w</span>
<span class="sd">                builder.variables[b.name] = b</span>

<span class="sd">                builder.tensor = tf.nn.sigmoid(tf.matmul(builder.tensor, w) + b)</span>

<span class="sd">                return builder</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .then(lambda builder: sigmoid_layer(builder, 3))</span>
<span class="sd">            )</span>

<span class="sd">        Note that the previous if equivalent to</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Expects a function **fn** with type `Builder -&gt; list( Builder | BuilderTree )`. This method enables you to *branch* the computational graph so you can easily create neural networks with more complex topologies. You can later</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `Builder -&gt; list( Builder | BuilderTree )`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.BuilderTree`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        The following will create a sigmoid layer but will branch the computation at the logit (z) so you get both the output tensor `h` and `trainer` tensor. Observe that first the logit `z` is calculated by creating a linear layer with `connect_layer(1)` and then its branched out</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h, trainer] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .tensors()</span>
<span class="sd">            )</span>

<span class="sd">        Note that you have to use the `tensorbuilder.builders.BuilderTree.tensors` method from the `tensorbuilder.builders.BuilderTree` class to get the tensors back. Remember that you can also contain `tensorbuilder.builders.BuilderTree` elements when you branch out, this means that you can keep branching inside branch. Don&#39;t worry that the tree keep getting deeper, `tensorbuilder.builders.BuilderTree` has methods that help you flatten or reduce the tree.</span>

<span class="sd">        The following example will show you how create a (overly) complex tree and then connect all the leaf nodes to a single `sigmoid` layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(10)</span>
<span class="sd">                .branch(lambda base:</span>
<span class="sd">                [</span>
<span class="sd">                    base</span>
<span class="sd">                    .connect_layer(3, fn=tf.nn.relu)</span>
<span class="sd">                ,</span>
<span class="sd">                    base</span>
<span class="sd">                    .connect_layer(9, fn=tf.nn.tanh)</span>
<span class="sd">                    .branch(lambda base2:</span>
<span class="sd">                    [</span>
<span class="sd">                        base2</span>
<span class="sd">                        .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">                    ,</span>
<span class="sd">                        base2</span>
<span class="sd">                        .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">                        .connect_layer(8, tf.nn.softmax)</span>
<span class="sd">                    ])</span>
<span class="sd">                ])</span>
<span class="sd">                .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        ** See Also **</span>

<span class="sd">        * `tensorbuilder.builders.BuilderTree`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.connect_layer`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.builders`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.tensors`</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">branches</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">_leafs</span><span class="p">(</span><span class="n">builder</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A generator function that yields the builder, used by `tensorbuilder.builders.BuilderTree.leafs` of `tensorbuilder.builders.BuilderTree`&quot;&quot;&quot;</span>
        <span class="k">yield</span> <span class="n">builder</span>


<span class="k">class</span> <span class="nc">BuilderTree</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BuilderTree is a class that enable you to perform computations over a complex branched builder. It contains methods to get all the leaf `tensorbuilder.builders.Builder` nodes, connect all the leaf nodes to a single layer, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">branches</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BuilderTree</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">branches</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A list that can contain elements that are of type `tensorbuilder.builders.Builder` or `tensorbuilder.builders.BuilderTree`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">branches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">[:]</span>
        <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">builders</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a flattened list `tensorbuilder.builders.Builder`s contained by this tree. The whole result is flattened in case of sub-elements are also `tensorbuilder.builders.BuilderTree`s.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tensorbuilder.builders.Builder )`</span>

<span class="sd">        ** Example **</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h_builder, trainer_builder] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .builders()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">builder</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>

    <span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as `tensorbuilder.builders.BuilderTree.builders` but extracts the tensor from each `tensorbuilder.builders.Builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tf.Tensor )`</span>

<span class="sd">        ** Example **</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h_tensor, trainer_tensor] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .tensors()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Connects all the leaf `tensorbuilder.builders.Builder` nodes of this tree to a single layer. The order of computation is done as follows</span>

<span class="sd">        1. Each leaf builder node is linearly connected to a layer of size `size` with no bias.</span>
<span class="sd">        2. All these layers of size `size` are added together (reduced with +)</span>
<span class="sd">        3. If `bias` is `True` then a bias added</span>
<span class="sd">        4. If `fn` is not `None` then the function `fn` is mapped</span>

<span class="sd">        ** Parameters **</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">        * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">        * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>

<span class="sd">        ** Examples **</span>

<span class="sd">        # The following example shows you how to connect two tensors (rather builders) of different shapes to a single `softmax` layer of shape [None, 3]</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            a = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>
<span class="sd">            b = tf.placeholder(tf.float32, shape=[None, 5]).builder()</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.branches([a, b])</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>

<span class="sd">        The next example show you how you can use this to pass the input layer directly through one branch, and &quot;analyze&quot; it with a `tanh layer` filter through the other, both of these are connect to a single `softmax` output layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .branch(lambda x:</span>
<span class="sd">                [</span>
<span class="sd">                    x</span>
<span class="sd">                ,</span>
<span class="sd">                    x.connect_layer(10, fn=tf.nn.tanh)</span>
<span class="sd">                ])</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">builders</span> <span class="o">=</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">_add_builders</span><span class="p">(</span><span class="n">builders</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
            <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>


    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">_leafs</span><span class="p">(</span><span class="n">tree</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A generator function that lazily returns all the Builders contianed by this tree&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">branch</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">branches</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">branch</span><span class="o">.</span><span class="n">_leafs</span><span class="p">():</span>
                <span class="k">yield</span> <span class="n">builder</span>




<span class="c1">## Module Funs</span>
<span class="k">def</span> <span class="nf">builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a tensor and returns a `tensorbuilder.builders.Builder` that contians it. If function is also used to monkey-patch tensorflow&#39;s Tensor class with a method of the same name.</span>

<span class="sd">    ** Parameters **</span>

<span class="sd">    * `tensor`: a tensorflow Tensor</span>

<span class="sd">    #### Example</span>

<span class="sd">    The following example shows you how to construct a `tensorbuilder.builders.Builder` from a tensorflow Tensor.</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>

<span class="sd">        a = tf.placeholder(tf.float32, shape=[None, 8])</span>
<span class="sd">        a_builder = tb.builder(a)</span>

<span class="sd">    The previous is the same as</span>

<span class="sd">        a_builder = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>

<span class="sd">    since tensorbuilder monkey-patches tensorflow&#39;s Tensor with this function as method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">branches</span><span class="p">(</span><span class="n">builder_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list with elements of type `tensorbuilder.builders.Builder` or `tensorbuilder.builders.BuilderTree` and returns a `tensorbuilder.builders.BuilderTree`</span>

<span class="sd">    ** Parameters **</span>

<span class="sd">    * `builder_list`: list of type `list( Builder | BuilderTree)`</span>

<span class="sd">    #### Example</span>

<span class="sd">    Given a list of Builders and/or BuilderTrees you construct a `tensorbuilder.builders.BuilderTree` like this</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>

<span class="sd">        a = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>
<span class="sd">        b = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>

<span class="sd">        tree = tb.branches([a, b])</span>

<span class="sd">    `tensorbuilder.builders.BuilderTree`s are usually constructed using `tensorbuilder.builders.Builder.branch` of the `tensorbuilder.builders.Builder` class, but you can use this for special cases</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">builder_list</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_add_builders</span><span class="p">(</span><span class="n">builders</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">variables</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">builders</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">+=</span> <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span>

        <span class="n">variables</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="n">variables</span><span class="p">)</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.branches">
    <p>def <span class="ident">branches</span>(</p><p>builder_list)</p>
    </div>
    

    
  
    <div class="desc"><p>Takes a list with elements of type <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> or <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a> and returns a <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a></p>
<p><strong> Parameters </strong></p>
<ul>
<li><code>builder_list</code>: list of type <code>list( Builder | BuilderTree)</code></li>
</ul>
<h4>Example</h4>
<p>Given a list of Builders and/or BuilderTrees you construct a <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a> like this</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">branches</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</pre></div>


<p><a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a>s are usually constructed using <a href="#tensorbuilder.builders.Builder.branch"><code>branch</code></a> of the <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> class, but you can use this for special cases</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.branches', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.branches" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">branches</span><span class="p">(</span><span class="n">builder_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list with elements of type `tensorbuilder.builders.Builder` or `tensorbuilder.builders.BuilderTree` and returns a `tensorbuilder.builders.BuilderTree`</span>

<span class="sd">    ** Parameters **</span>

<span class="sd">    * `builder_list`: list of type `list( Builder | BuilderTree)`</span>

<span class="sd">    #### Example</span>

<span class="sd">    Given a list of Builders and/or BuilderTrees you construct a `tensorbuilder.builders.BuilderTree` like this</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>

<span class="sd">        a = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>
<span class="sd">        b = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>

<span class="sd">        tree = tb.branches([a, b])</span>

<span class="sd">    `tensorbuilder.builders.BuilderTree`s are usually constructed using `tensorbuilder.builders.Builder.branch` of the `tensorbuilder.builders.Builder` class, but you can use this for special cases</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">builder_list</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.builder">
    <p>def <span class="ident">builder</span>(</p><p>tensor)</p>
    </div>
    

    
  
    <div class="desc"><p>Takes a tensor and returns a <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> that contians it. If function is also used to monkey-patch tensorflow's Tensor class with a method of the same name.</p>
<p><strong> Parameters </strong></p>
<ul>
<li><code>tensor</code>: a tensorflow Tensor</li>
</ul>
<h4>Example</h4>
<p>The following example shows you how to construct a <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> from a tensorflow Tensor.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">a_builder</span> <span class="o">=</span> <span class="n">tb</span><span class="o">.</span><span class="n">builder</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>


<p>The previous is the same as</p>
<div class="codehilite"><pre><span></span>a_builder = tf.placeholder(tf.float32, shape=[None, 8]).builder()
</pre></div>


<p>since tensorbuilder monkey-patches tensorflow's Tensor with this function as method.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.builder', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.builder" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a tensor and returns a `tensorbuilder.builders.Builder` that contians it. If function is also used to monkey-patch tensorflow&#39;s Tensor class with a method of the same name.</span>

<span class="sd">    ** Parameters **</span>

<span class="sd">    * `tensor`: a tensorflow Tensor</span>

<span class="sd">    #### Example</span>

<span class="sd">    The following example shows you how to construct a `tensorbuilder.builders.Builder` from a tensorflow Tensor.</span>

<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>

<span class="sd">        a = tf.placeholder(tf.float32, shape=[None, 8])</span>
<span class="sd">        a_builder = tb.builder(a)</span>

<span class="sd">    The previous is the same as</span>

<span class="sd">        a_builder = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>

<span class="sd">    since tensorbuilder monkey-patches tensorflow&#39;s Tensor with this function as method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  

    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="tensorbuilder.builders.Builder" class="name">class <span class="ident">Builder</span></p>
      
  
    <div class="desc"><p>The Builder class is a wrapper around a Tensor. Most of its method are immutable in the sense that they don't modify the caller object but rather always make a copy, they also tend to return a Builder so you you can keep fluently chaining methods.</p>
<p>To create a builder from a method you have these options:</p>
<ol>
<li>
<p>Use the <a href="#tensorbuilder.builders.builder"><code>builder</code></a> function</p>
<div class="codehilite"><pre><span></span>tb.builder(tensor)
</pre></div>


</li>
<li>
<p>Use the monkey-patched method on the Tensor class</p>
<div class="codehilite"><pre><span></span>tensor.builder()
</pre></div>


</li>
</ol></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Builder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Builder class is a wrapper around a Tensor. Most of its method are immutable in the sense that they don&#39;t modify the caller object but rather always make a copy, they also tend to return a Builder so you you can keep fluently chaining methods.</span>

<span class="sd">    To create a builder from a method you have these options:</span>

<span class="sd">    1. Use the `tensorbuilder.builders.builder` function</span>

<span class="sd">            tb.builder(tensor)</span>

<span class="sd">    2. Use the monkey-patched method on the Tensor class</span>

<span class="sd">            tensor.builder()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">{}):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="sd">&quot;&quot;&quot;A `tensorflow` Tensor.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">variables</span> <span class="o">=</span> <span class="n">variables</span>
        <span class="sd">&quot;&quot;&quot;A dictionary that accumulates the **tf.Variable** tensors generated during the building process, it has the form {tensor_name: String -&gt; tensor: tf.Variable}. Since functions like `tensorbuilder.builders.Builder.connect_layer` hides you the complexity of creating the **bias** and **weights** of your network, `tensorbuilder.builders.Builder` stores them in this field. The methods of this class enables you to set the names of these variables, but take into account that the final name is actually the name of the tensor, with is set by `tensorflow`. Check their documentation to see how the name is defined.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns a copy of this Builder&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_weights</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **w** be a **tf.Variable** of shape **[n, size]**. Then `builder.connect_weights(size)` computes `tf.matmul(x, w)`.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">        * `name`: the name of the tensor (default: &quot;connect_weights&quot;)</span>
<span class="sd">        * `weights_name`: the name of the **w** tensor of type `tf.Variable`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds `tf.matmul(x, w)`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            z = x.builder().connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">size</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">weights_name</span> <span class="k">if</span> <span class="n">weights_name</span> <span class="k">else</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span>

        <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_bias</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **b** be a **tf.Variable** of shape **[n]**. Then `builder.connect_bias()` computes `tf.add(x, b)`.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **b** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `name`: the name of the tensor (default: &quot;connect_bias&quot;)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: &quot;b&quot;).</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds `tf.matmul(x, w) + b`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            z = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">                .connect_bias(bias_name=&quot;bias&quot;)</span>
<span class="sd">            )</span>

<span class="sd">        Note, the previous is equivalent to using `tensorbuilder.builders.Builder.connect_layer` like this</span>

<span class="sd">            z = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, weights_name=&quot;weights&quot;, bias_name=&quot;bias&quot;)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>
        <span class="n">var_name</span> <span class="o">=</span> <span class="n">bias_name</span> <span class="k">if</span> <span class="n">bias_name</span> <span class="k">else</span> <span class="n">b</span><span class="o">.</span><span class="n">name</span>

        <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>


    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, let **w** be a **tf.Variable** of shape **[n, size]**, let **b** be a **tf.Variable** of shape **[n]**, and **fn** be a function from a tensor to a tensor. Then `builder.connect_layer(size, fn=fn)` computes `fn(tf.matmul(x, w) + b)`. If **fn** is not present the layer is linear.</span>

<span class="sd">        Note that **fn** must expose the keyword/named argument `name`, this is compatible with the tensorflow API.</span>

<span class="sd">        The returned `tensorbuilder.builders.Builder` has **b** and **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">        * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">        * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">        * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">        * `weights_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following builds the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)`</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        The previous is equivalent to using</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_weights(3)</span>
<span class="sd">                .connect_bias()</span>
<span class="sd">                .map(tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        You can chain various `connect_layer`s to get deeper neural networks</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">                .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>


        <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
            <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Let **x** be `tensorbuilder.builders.Builder.tensor` and **fn** be a function from a tensor to a tensor. Then `builder.map(fn)` computes `fn(x)`. All extra positional and named arguments are forwarded to **fn** such that</span>

<span class="sd">            builder.map(fn, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>

<span class="sd">        internally results in</span>

<span class="sd">            builder.tensor = fn(builder.tensor, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        **Examples**</span>

<span class="sd">        The following constructs a neural network with the architecture `[40 input, 100 tanh, 30 softmax]` and and applies `dropout` to the tanh layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">                .map(tb.nn.dropout, keep_prob)</span>
<span class="sd">                .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">builder</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">then</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Expects a function **fn** with type `builder -&gt; builder`. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `builder -&gt; builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.Builder`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        The following *manually* constructs the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)` while updating the `tensorbuilder.tensorbuiler.Builder.variables` dictionary.</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            def sigmoid_layer(builder, size):</span>
<span class="sd">                m = int(builder.tensor.get_shape()[1])</span>
<span class="sd">                n = size</span>

<span class="sd">                w = tf.Variable(tf.random_uniform([m, n], -1.0, 1.0))</span>
<span class="sd">                b = tf.Variable(tf.random_uniform([n], -1.0, 1.0))</span>

<span class="sd">                builder.variables[w.name] = w</span>
<span class="sd">                builder.variables[b.name] = b</span>

<span class="sd">                builder.tensor = tf.nn.sigmoid(tf.matmul(builder.tensor, w) + b)</span>

<span class="sd">                return builder</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .then(lambda builder: sigmoid_layer(builder, 3))</span>
<span class="sd">            )</span>

<span class="sd">        Note that the previous if equivalent to</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `@_immutable`</span>

<span class="sd">        Expects a function **fn** with type `Builder -&gt; list( Builder | BuilderTree )`. This method enables you to *branch* the computational graph so you can easily create neural networks with more complex topologies. You can later</span>

<span class="sd">        **Parameters**</span>

<span class="sd">        * `fn`: a function of type `Builder -&gt; list( Builder | BuilderTree )`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `tensorbuilder.builders.BuilderTree`</span>

<span class="sd">        ** Example **</span>

<span class="sd">        The following will create a sigmoid layer but will branch the computation at the logit (z) so you get both the output tensor `h` and `trainer` tensor. Observe that first the logit `z` is calculated by creating a linear layer with `connect_layer(1)` and then its branched out</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h, trainer] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .tensors()</span>
<span class="sd">            )</span>

<span class="sd">        Note that you have to use the `tensorbuilder.builders.BuilderTree.tensors` method from the `tensorbuilder.builders.BuilderTree` class to get the tensors back. Remember that you can also contain `tensorbuilder.builders.BuilderTree` elements when you branch out, this means that you can keep branching inside branch. Don&#39;t worry that the tree keep getting deeper, `tensorbuilder.builders.BuilderTree` has methods that help you flatten or reduce the tree.</span>

<span class="sd">        The following example will show you how create a (overly) complex tree and then connect all the leaf nodes to a single `sigmoid` layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            keep_prob = tf.placeholder(tf.float32)</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(10)</span>
<span class="sd">                .branch(lambda base:</span>
<span class="sd">                [</span>
<span class="sd">                    base</span>
<span class="sd">                    .connect_layer(3, fn=tf.nn.relu)</span>
<span class="sd">                ,</span>
<span class="sd">                    base</span>
<span class="sd">                    .connect_layer(9, fn=tf.nn.tanh)</span>
<span class="sd">                    .branch(lambda base2:</span>
<span class="sd">                    [</span>
<span class="sd">                        base2</span>
<span class="sd">                        .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">                    ,</span>
<span class="sd">                        base2</span>
<span class="sd">                        .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">                        .connect_layer(8, tf.nn.softmax)</span>
<span class="sd">                    ])</span>
<span class="sd">                ])</span>
<span class="sd">                .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">            )</span>

<span class="sd">        ** See Also **</span>

<span class="sd">        * `tensorbuilder.builders.BuilderTree`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.connect_layer`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.builders`</span>
<span class="sd">        * `tensorbuilder.builders.BuilderTree.tensors`</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">branches</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">_leafs</span><span class="p">(</span><span class="n">builder</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A generator function that yields the builder, used by `tensorbuilder.builders.BuilderTree.leafs` of `tensorbuilder.builders.BuilderTree`&quot;&quot;&quot;</span>
        <span class="k">yield</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.builders.Builder">Builder</a></li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="tensorbuilder.builders.Builder.tensor" class="name">var <span class="ident">tensor</span></p>
            

            
  
    <div class="desc"><p>A <code>tensorflow</code> Tensor.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="tensorbuilder.builders.Builder.variables" class="name">var <span class="ident">variables</span></p>
            

            
  
    <div class="desc"><p>A dictionary that accumulates the <strong>tf.Variable</strong> tensors generated during the building process, it has the form {tensor_name: String -&gt; tensor: tf.Variable}. Since functions like <a href="#tensorbuilder.builders.Builder.connect_layer"><code>connect_layer</code></a> hides you the complexity of creating the <strong>bias</strong> and <strong>weights</strong> of your network, <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> stores them in this field. The methods of this class enables you to set the names of these variables, but take into account that the final name is actually the name of the tensor, with is set by <code>tensorflow</code>. Check their documentation to see how the name is defined.</p></div>
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, tensor, variables={})</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.__init__', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">{}):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Builder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
    <span class="sd">&quot;&quot;&quot;A `tensorflow` Tensor.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variables</span> <span class="o">=</span> <span class="n">variables</span>
    <span class="sd">&quot;&quot;&quot;A dictionary that accumulates the **tf.Variable** tensors generated during the building process, it has the form {tensor_name: String -&gt; tensor: tf.Variable}. Since functions like `tensorbuilder.builders.Builder.connect_layer` hides you the complexity of creating the **bias** and **weights** of your network, `tensorbuilder.builders.Builder` stores them in this field. The methods of this class enables you to set the names of these variables, but take into account that the final name is actually the name of the tensor, with is set by `tensorflow`. Check their documentation to see how the name is defined.&quot;&quot;&quot;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.branch">
    <p>def <span class="ident">branch</span>(</p><p>builder, fn)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>Builder -&gt; list( Builder | BuilderTree )</code>. This method enables you to <em>branch</em> the computational graph so you can easily create neural networks with more complex topologies. You can later</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>Builder -&gt; list( Builder | BuilderTree )</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a></li>
</ul>
<p><strong> Example </strong></p>
<p>The following will create a sigmoid layer but will branch the computation at the logit (z) so you get both the output tensor <code>h</code> and <code>trainer</code> tensor. Observe that first the logit <code>z</code> is calculated by creating a linear layer with <code>connect_layer(1)</code> and then its branched out</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">trainer</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">tensors</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Note that you have to use the <a href="#tensorbuilder.builders.BuilderTree.tensors"><code>tensors</code></a> method from the <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a> class to get the tensors back. Remember that you can also contain <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a> elements when you branch out, this means that you can keep branching inside branch. Don't worry that the tree keep getting deeper, <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a> has methods that help you flatten or reduce the tree.</p>
<p>The following example will show you how create a (overly) complex tree and then connect all the leaf nodes to a single <code>sigmoid</code> layer</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">base</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">base</span>
        <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">base</span>
        <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
        <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">base2</span><span class="p">:</span>
        <span class="p">[</span>
            <span class="n">base2</span>
            <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
        <span class="p">,</span>
            <span class="n">base2</span>
            <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
            <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
        <span class="p">])</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>


<p><strong> See Also </strong></p>
<ul>
<li><a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a></li>
<li><a href="#tensorbuilder.builders.BuilderTree.connect_layer"><code>connect_layer</code></a></li>
<li><a href="#tensorbuilder.builders.BuilderTree.builders"><code>builders</code></a></li>
<li><a href="#tensorbuilder.builders.BuilderTree.tensors"><code>tensors</code></a></li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.branch', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.branch" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">branch</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Expects a function **fn** with type `Builder -&gt; list( Builder | BuilderTree )`. This method enables you to *branch* the computational graph so you can easily create neural networks with more complex topologies. You can later</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `Builder -&gt; list( Builder | BuilderTree )`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.BuilderTree`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    The following will create a sigmoid layer but will branch the computation at the logit (z) so you get both the output tensor `h` and `trainer` tensor. Observe that first the logit `z` is calculated by creating a linear layer with `connect_layer(1)` and then its branched out</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        [h, trainer] = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(1)</span>
<span class="sd">            .branch(lambda z:</span>
<span class="sd">            [</span>
<span class="sd">                z.map(tf.nn.sigmoid)</span>
<span class="sd">            ,</span>
<span class="sd">                z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">            ])</span>
<span class="sd">            .tensors()</span>
<span class="sd">        )</span>
<span class="sd">    Note that you have to use the `tensorbuilder.builders.BuilderTree.tensors` method from the `tensorbuilder.builders.BuilderTree` class to get the tensors back. Remember that you can also contain `tensorbuilder.builders.BuilderTree` elements when you branch out, this means that you can keep branching inside branch. Don&#39;t worry that the tree keep getting deeper, `tensorbuilder.builders.BuilderTree` has methods that help you flatten or reduce the tree.</span>
<span class="sd">    The following example will show you how create a (overly) complex tree and then connect all the leaf nodes to a single `sigmoid` layer</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        keep_prob = tf.placeholder(tf.float32)</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(10)</span>
<span class="sd">            .branch(lambda base:</span>
<span class="sd">            [</span>
<span class="sd">                base</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.relu)</span>
<span class="sd">            ,</span>
<span class="sd">                base</span>
<span class="sd">                .connect_layer(9, fn=tf.nn.tanh)</span>
<span class="sd">                .branch(lambda base2:</span>
<span class="sd">                [</span>
<span class="sd">                    base2</span>
<span class="sd">                    .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    base2</span>
<span class="sd">                    .map(tf.nn.dropout, keep_prob)</span>
<span class="sd">                    .connect_layer(8, tf.nn.softmax)</span>
<span class="sd">                ])</span>
<span class="sd">            ])</span>
<span class="sd">            .connect_layer(6, fn=tf.nn.sigmoid)</span>
<span class="sd">        )</span>
<span class="sd">    ** See Also **</span>
<span class="sd">    * `tensorbuilder.builders.BuilderTree`</span>
<span class="sd">    * `tensorbuilder.builders.BuilderTree.connect_layer`</span>
<span class="sd">    * `tensorbuilder.builders.BuilderTree.builders`</span>
<span class="sd">    * `tensorbuilder.builders.BuilderTree.tensors`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">branches</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.connect_bias">
    <p>def <span class="ident">connect_bias</span>(</p><p>builder, name=None, bias_name=None)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Let <strong>x</strong> be <a href="#tensorbuilder.builders.Builder.tensor"><code>tensor</code></a> of shape <strong>[m, n]</strong>, and let <strong>b</strong> be a <strong>tf.Variable</strong> of shape <strong>[n]</strong>. Then <code>builder.connect_bias()</code> computes <code>tf.add(x, b)</code>.</p>
<p>The returned <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> has <strong>b</strong> stored inside <a href="#tensorbuilder.builders.Builder.variables"><code>variables</code></a>.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>name</code>: the name of the tensor (default: "connect_bias")</li>
<li><code>bias_name</code>: the name of the <code>w</code> tensor of type <code>tf.Variable</code> (default: "b").</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.Builder"><code>Builder</code></a></li>
</ul>
<p><strong>Examples</strong></p>
<p>The following builds <code>tf.matmul(x, w) + b</code></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">z</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="s2">&quot;bias&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>


<p>Note, the previous is equivalent to using <a href="#tensorbuilder.builders.Builder.connect_layer"><code>connect_layer</code></a> like this</p>
<div class="codehilite"><pre><span></span>z = (
    x.builder()
    .connect_layer(3, weights_name=&quot;weights&quot;, bias_name=&quot;bias&quot;)
)
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.connect_bias', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.connect_bias" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">connect_bias</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **b** be a **tf.Variable** of shape **[n]**. Then `builder.connect_bias()` computes `tf.add(x, b)`.</span>
<span class="sd">    The returned `tensorbuilder.builders.Builder` has **b** stored inside `tensorbuilder.builders.Builder.variables`.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `name`: the name of the tensor (default: &quot;connect_bias&quot;)</span>
<span class="sd">    * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: &quot;b&quot;).</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.Builder`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    The following builds `tf.matmul(x, w) + b`</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        z = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">            .connect_bias(bias_name=&quot;bias&quot;)</span>
<span class="sd">        )</span>
<span class="sd">    Note, the previous is equivalent to using `tensorbuilder.builders.Builder.connect_layer` like this</span>
<span class="sd">        z = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(3, weights_name=&quot;weights&quot;, bias_name=&quot;bias&quot;)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>
    <span class="n">var_name</span> <span class="o">=</span> <span class="n">bias_name</span> <span class="k">if</span> <span class="n">bias_name</span> <span class="k">else</span> <span class="n">b</span><span class="o">.</span><span class="n">name</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.connect_layer">
    <p>def <span class="ident">connect_layer</span>(</p><p>builder, size, fn=None, name=None, weights_name=None, bias=True, bias_name=None)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Let <strong>x</strong> be <a href="#tensorbuilder.builders.Builder.tensor"><code>tensor</code></a> of shape <strong>[m, n]</strong>, let <strong>w</strong> be a <strong>tf.Variable</strong> of shape <strong>[n, size]</strong>, let <strong>b</strong> be a <strong>tf.Variable</strong> of shape <strong>[n]</strong>, and <strong>fn</strong> be a function from a tensor to a tensor. Then <code>builder.connect_layer(size, fn=fn)</code> computes <code>fn(tf.matmul(x, w) + b)</code>. If <strong>fn</strong> is not present the layer is linear.</p>
<p>Note that <strong>fn</strong> must expose the keyword/named argument <code>name</code>, this is compatible with the tensorflow API.</p>
<p>The returned <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> has <strong>b</strong> and <strong>w</strong> stored inside <a href="#tensorbuilder.builders.Builder.variables"><code>variables</code></a>.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>tensor -&gt; tensor</code>. If <code>fn</code> is <code>None</code> then its not applied, resulting in just a linear trasformation. (default: None)</li>
<li><code>size</code>: an <code>int</code> representing the size of the layer (number of neurons)</li>
<li><code>name</code>: the name of the tensor (default: <code>"layer"</code>)</li>
<li><code>bias</code>: determines where to use a bias <strong>b</strong> or not (default: <code>True</code>)</li>
<li><code>weights_name</code>: the name of the <code>w</code> tensor of type <code>tf.Variable</code> (default: <code>None</code>)</li>
<li><code>bias_name</code>: the name of the <code>w</code> tensor of type <code>tf.Variable</code> (default: <code>None</code>)</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.Builder"><code>Builder</code></a></li>
</ul>
<p><strong>Examples</strong></p>
<p>The following builds the computation <code>tf.nn.sigmoid(tf.matmul(x, w) + b)</code></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>


<p>The previous is equivalent to using</p>
<div class="codehilite"><pre><span></span>h = (
    x.builder()
    .connect_weights(3)
    .connect_bias()
    .map(tf.nn.sigmoid)
)
</pre></div>


<p>You can chain various <code>connect_layer</code>s to get deeper neural networks</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.connect_layer', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.connect_layer" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, let **w** be a **tf.Variable** of shape **[n, size]**, let **b** be a **tf.Variable** of shape **[n]**, and **fn** be a function from a tensor to a tensor. Then `builder.connect_layer(size, fn=fn)` computes `fn(tf.matmul(x, w) + b)`. If **fn** is not present the layer is linear.</span>
<span class="sd">    Note that **fn** must expose the keyword/named argument `name`, this is compatible with the tensorflow API.</span>
<span class="sd">    The returned `tensorbuilder.builders.Builder` has **b** and **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">    * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">    * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">    * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">    * `weights_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>
<span class="sd">    * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.Builder`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    The following builds the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)`</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">        )</span>
<span class="sd">    The previous is equivalent to using</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_weights(3)</span>
<span class="sd">            .connect_bias()</span>
<span class="sd">            .map(tf.nn.sigmoid)</span>
<span class="sd">        )</span>
<span class="sd">    You can chain various `connect_layer`s to get deeper neural networks</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">            .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.connect_weights">
    <p>def <span class="ident">connect_weights</span>(</p><p>builder, size, name=None, weights_name=None)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Let <strong>x</strong> be <a href="#tensorbuilder.builders.Builder.tensor"><code>tensor</code></a> of shape <strong>[m, n]</strong>, and let <strong>w</strong> be a <strong>tf.Variable</strong> of shape <strong>[n, size]</strong>. Then <code>builder.connect_weights(size)</code> computes <code>tf.matmul(x, w)</code>.</p>
<p>The returned <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> has <strong>w</strong> stored inside <a href="#tensorbuilder.builders.Builder.variables"><code>variables</code></a>.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>size</code>: an <code>int</code> representing the size of the layer (number of neurons)</li>
<li><code>name</code>: the name of the tensor (default: "connect_weights")</li>
<li><code>weights_name</code>: the name of the <strong>w</strong> tensor of type <code>tf.Variable</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.Builder"><code>Builder</code></a></li>
</ul>
<p><strong>Examples</strong></p>
<p>The following builds <code>tf.matmul(x, w)</code></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.connect_weights', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.connect_weights" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">connect_weights</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Let **x** be `tensorbuilder.builders.Builder.tensor` of shape **[m, n]**, and let **w** be a **tf.Variable** of shape **[n, size]**. Then `builder.connect_weights(size)` computes `tf.matmul(x, w)`.</span>
<span class="sd">    The returned `tensorbuilder.builders.Builder` has **w** stored inside `tensorbuilder.builders.Builder.variables`.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `size`: an `int` representing the size of the layer (number of neurons)</span>
<span class="sd">    * `name`: the name of the tensor (default: &quot;connect_weights&quot;)</span>
<span class="sd">    * `weights_name`: the name of the **w** tensor of type `tf.Variable`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.Builder`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    The following builds `tf.matmul(x, w)`</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        z = x.builder().connect_weights(3, weights_name=&quot;weights&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">size</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">weights_name</span><span class="p">)</span>
    <span class="n">var_name</span> <span class="o">=</span> <span class="n">weights_name</span> <span class="k">if</span> <span class="n">weights_name</span> <span class="k">else</span> <span class="n">w</span><span class="o">.</span><span class="n">name</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">var_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.copy">
    <p>def <span class="ident">copy</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a copy of this Builder</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.copy', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this Builder&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Builder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.map">
    <p>def <span class="ident">map</span>(</p><p>builder, fn, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Let <strong>x</strong> be <a href="#tensorbuilder.builders.Builder.tensor"><code>tensor</code></a> and <strong>fn</strong> be a function from a tensor to a tensor. Then <code>builder.map(fn)</code> computes <code>fn(x)</code>. All extra positional and named arguments are forwarded to <strong>fn</strong> such that</p>
<div class="codehilite"><pre><span></span>builder.map(fn, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)
</pre></div>


<p>internally results in</p>
<div class="codehilite"><pre><span></span>builder.tensor = fn(builder.tensor, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)
</pre></div>


<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>tensor -&gt; tensor</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.Builder"><code>Builder</code></a></li>
</ul>
<p><strong>Examples</strong></p>
<p>The following constructs a neural network with the architecture <code>[40 input, 100 tanh, 30 softmax]</code> and and applies <code>dropout</code> to the tanh layer</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tb</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.map', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.map" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Let **x** be `tensorbuilder.builders.Builder.tensor` and **fn** be a function from a tensor to a tensor. Then `builder.map(fn)` computes `fn(x)`. All extra positional and named arguments are forwarded to **fn** such that</span>
<span class="sd">        builder.map(fn, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>
<span class="sd">    internally results in</span>
<span class="sd">        builder.tensor = fn(builder.tensor, arg1, arg2, ..., kwarg1=kwarg1, kwarg2=kwarg2, ...)</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `tensor -&gt; tensor`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.Builder`</span>
<span class="sd">    **Examples**</span>
<span class="sd">    The following constructs a neural network with the architecture `[40 input, 100 tanh, 30 softmax]` and and applies `dropout` to the tanh layer</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">        keep_prob = tf.placeholder(tf.float32)</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(100, fn=tf.nn.tanh)</span>
<span class="sd">            .map(tb.nn.dropout, keep_prob)</span>
<span class="sd">            .connect_layer(30, fn=tf.nn.softmax)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.Builder.then">
    <p>def <span class="ident">then</span>(</p><p>builder, fn)</p>
    </div>
    

    
  
    <div class="desc"><p><code>@_immutable</code></p>
<p>Expects a function <strong>fn</strong> with type <code>builder -&gt; builder</code>. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</p>
<p><strong>Parameters</strong></p>
<ul>
<li><code>fn</code>: a function of type <code>builder -&gt; builder</code>.</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><a href="#tensorbuilder.builders.Builder"><code>Builder</code></a></li>
</ul>
<p><strong> Example </strong></p>
<p>The following <em>manually</em> constructs the computation <code>tf.nn.sigmoid(tf.matmul(x, w) + b)</code> while updating the <code>tensorbuilder.tensorbuiler.Builder.variables</code> dictionary.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid_layer</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">size</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

    <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>

    <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">builder</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="k">lambda</span> <span class="n">builder</span><span class="p">:</span> <span class="n">sigmoid_layer</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>


<p>Note that the previous if equivalent to</p>
<div class="codehilite"><pre><span></span>h = (
    x.builder()
    .connect_layer(3, fn=tf.nn.sigmoid)
)
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.Builder.then', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.Builder.then" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">then</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `@_immutable`</span>
<span class="sd">    Expects a function **fn** with type `builder -&gt; builder`. This method is used primarily to manupilate the Builder with very fine grain control through the fluent immutable API.</span>
<span class="sd">    **Parameters**</span>
<span class="sd">    * `fn`: a function of type `builder -&gt; builder`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `tensorbuilder.builders.Builder`</span>
<span class="sd">    ** Example **</span>
<span class="sd">    The following *manually* constructs the computation `tf.nn.sigmoid(tf.matmul(x, w) + b)` while updating the `tensorbuilder.tensorbuiler.Builder.variables` dictionary.</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 40])</span>
<span class="sd">        keep_prob = tf.placeholder(tf.float32)</span>
<span class="sd">        def sigmoid_layer(builder, size):</span>
<span class="sd">            m = int(builder.tensor.get_shape()[1])</span>
<span class="sd">            n = size</span>
<span class="sd">            w = tf.Variable(tf.random_uniform([m, n], -1.0, 1.0))</span>
<span class="sd">            b = tf.Variable(tf.random_uniform([n], -1.0, 1.0))</span>
<span class="sd">            builder.variables[w.name] = w</span>
<span class="sd">            builder.variables[b.name] = b</span>
<span class="sd">            builder.tensor = tf.nn.sigmoid(tf.matmul(builder.tensor, w) + b)</span>
<span class="sd">            return builder</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .then(lambda builder: sigmoid_layer(builder, 3))</span>
<span class="sd">        )</span>
<span class="sd">    Note that the previous if equivalent to</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(3, fn=tf.nn.sigmoid)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="tensorbuilder.builders.BuilderTree" class="name">class <span class="ident">BuilderTree</span></p>
      
  
    <div class="desc"><p>BuilderTree is a class that enable you to perform computations over a complex branched builder. It contains methods to get all the leaf <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> nodes, connect all the leaf nodes to a single layer, etc.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">BuilderTree</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BuilderTree is a class that enable you to perform computations over a complex branched builder. It contains methods to get all the leaf `tensorbuilder.builders.Builder` nodes, connect all the leaf nodes to a single layer, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">branches</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BuilderTree</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">branches</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A list that can contain elements that are of type `tensorbuilder.builders.Builder` or `tensorbuilder.builders.BuilderTree`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">branches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">[:]</span>
        <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">builders</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a flattened list `tensorbuilder.builders.Builder`s contained by this tree. The whole result is flattened in case of sub-elements are also `tensorbuilder.builders.BuilderTree`s.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tensorbuilder.builders.Builder )`</span>

<span class="sd">        ** Example **</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h_builder, trainer_builder] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .builders()</span>
<span class="sd">            )</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">builder</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>

    <span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Same as `tensorbuilder.builders.BuilderTree.builders` but extracts the tensor from each `tensorbuilder.builders.Builder`.</span>

<span class="sd">        **Return**</span>

<span class="sd">        * `list( tf.Tensor )`</span>

<span class="sd">        ** Example **</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">            y = tf.placeholder(tf.float32, shape=[None, 1])</span>

<span class="sd">            [h_tensor, trainer_tensor] = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .connect_layer(1)</span>
<span class="sd">                .branch(lambda z:</span>
<span class="sd">                [</span>
<span class="sd">                    z.map(tf.nn.sigmoid)</span>
<span class="sd">                ,</span>
<span class="sd">                    z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                    .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">                ])</span>
<span class="sd">                .tensors()</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>

    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Connects all the leaf `tensorbuilder.builders.Builder` nodes of this tree to a single layer. The order of computation is done as follows</span>

<span class="sd">        1. Each leaf builder node is linearly connected to a layer of size `size` with no bias.</span>
<span class="sd">        2. All these layers of size `size` are added together (reduced with +)</span>
<span class="sd">        3. If `bias` is `True` then a bias added</span>
<span class="sd">        4. If `fn` is not `None` then the function `fn` is mapped</span>

<span class="sd">        ** Parameters **</span>

<span class="sd">        * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">        * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">        * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">        * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>

<span class="sd">        ** Examples **</span>

<span class="sd">        # The following example shows you how to connect two tensors (rather builders) of different shapes to a single `softmax` layer of shape [None, 3]</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            a = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>
<span class="sd">            b = tf.placeholder(tf.float32, shape=[None, 5]).builder()</span>

<span class="sd">            h = (</span>
<span class="sd">                tb.branches([a, b])</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>

<span class="sd">        The next example show you how you can use this to pass the input layer directly through one branch, and &quot;analyze&quot; it with a `tanh layer` filter through the other, both of these are connect to a single `softmax` output layer</span>

<span class="sd">            import tensorflow as tf</span>
<span class="sd">            import tensorbuilder as tb</span>

<span class="sd">            x = tf.placeholder(tf.float32, shape=[None, 5])</span>

<span class="sd">            h = (</span>
<span class="sd">                x.builder()</span>
<span class="sd">                .branch(lambda x:</span>
<span class="sd">                [</span>
<span class="sd">                    x</span>
<span class="sd">                ,</span>
<span class="sd">                    x.connect_layer(10, fn=tf.nn.tanh)</span>
<span class="sd">                ])</span>
<span class="sd">                .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">            )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">builders</span> <span class="o">=</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">_add_builders</span><span class="p">(</span><span class="n">builders</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
            <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">builder</span>


    <span class="nd">@_immutable</span>
    <span class="k">def</span> <span class="nf">_leafs</span><span class="p">(</span><span class="n">tree</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A generator function that lazily returns all the Builders contianed by this tree&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">branch</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">branches</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">branch</span><span class="o">.</span><span class="n">_leafs</span><span class="p">():</span>
                <span class="k">yield</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.builders.BuilderTree">BuilderTree</a></li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="tensorbuilder.builders.BuilderTree.branches" class="name">var <span class="ident">branches</span></p>
            

            
  
    <div class="desc"><p>A list that can contain elements that are of type <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> or <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a>.</p></div>
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.BuilderTree.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, branches)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree.__init__', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">branches</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BuilderTree</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">branches</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A list that can contain elements that are of type `tensorbuilder.builders.Builder` or `tensorbuilder.builders.BuilderTree`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.BuilderTree.builders">
    <p>def <span class="ident">builders</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns a flattened list <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a>s contained by this tree. The whole result is flattened in case of sub-elements are also <a href="#tensorbuilder.builders.BuilderTree"><code>BuilderTree</code></a>s.</p>
<p><strong>Return</strong></p>
<ul>
<li><code>list( tensorbuilder.builders.Builder )</code></li>
</ul>
<p><strong> Example </strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="p">[</span><span class="n">h_builder</span><span class="p">,</span> <span class="n">trainer_builder</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">builders</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree.builders', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree.builders" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">builders</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a flattened list `tensorbuilder.builders.Builder`s contained by this tree. The whole result is flattened in case of sub-elements are also `tensorbuilder.builders.BuilderTree`s.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `list( tensorbuilder.builders.Builder )`</span>
<span class="sd">    ** Example **</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        [h_builder, trainer_builder] = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(1)</span>
<span class="sd">            .branch(lambda z:</span>
<span class="sd">            [</span>
<span class="sd">                z.map(tf.nn.sigmoid)</span>
<span class="sd">            ,</span>
<span class="sd">                z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">            ])</span>
<span class="sd">            .builders()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">builder</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.BuilderTree.connect_layer">
    <p>def <span class="ident">connect_layer</span>(</p><p>tree, size, fn=None, name=&#39;layer&#39;, bias=True, bias_name=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Connects all the leaf <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a> nodes of this tree to a single layer. The order of computation is done as follows</p>
<ol>
<li>Each leaf builder node is linearly connected to a layer of size <code>size</code> with no bias.</li>
<li>All these layers of size <code>size</code> are added together (reduced with +)</li>
<li>If <code>bias</code> is <code>True</code> then a bias added</li>
<li>If <code>fn</code> is not <code>None</code> then the function <code>fn</code> is mapped</li>
</ol>
<p><strong> Parameters </strong></p>
<ul>
<li><code>fn</code>: a function of type <code>tensor -&gt; tensor</code>. If <code>fn</code> is <code>None</code> then its not applied, resulting in just a linear trasformation. (default: None)</li>
<li><code>name</code>: the name of the tensor (default: <code>"layer"</code>)</li>
<li><code>bias</code>: determines where to use a bias <strong>b</strong> or not (default: <code>True</code>)</li>
<li><code>bias_name</code>: the name of the <code>w</code> tensor of type <code>tf.Variable</code> (default: <code>None</code>)</li>
</ul>
<p><strong> Examples </strong></p>
<h1>The following example shows you how to connect two tensors (rather builders) of different shapes to a single <code>softmax</code> layer of shape [None, 3]</h1>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">branches</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>


<p>The next example show you how you can use this to pass the input layer directly through one branch, and "analyze" it with a <code>tanh layer</code> filter through the other, both of these are connect to a single <code>softmax</code> output layer</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">x</span>
    <span class="p">,</span>
        <span class="n">x</span><span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree.connect_layer', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree.connect_layer" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@_immutable</span>
<span class="k">def</span> <span class="nf">connect_layer</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bias_name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connects all the leaf `tensorbuilder.builders.Builder` nodes of this tree to a single layer. The order of computation is done as follows</span>
<span class="sd">    1. Each leaf builder node is linearly connected to a layer of size `size` with no bias.</span>
<span class="sd">    2. All these layers of size `size` are added together (reduced with +)</span>
<span class="sd">    3. If `bias` is `True` then a bias added</span>
<span class="sd">    4. If `fn` is not `None` then the function `fn` is mapped</span>
<span class="sd">    ** Parameters **</span>
<span class="sd">    * `fn`: a function of type `tensor -&gt; tensor`. If `fn` is `None` then its not applied, resulting in just a linear trasformation. (default: None)</span>
<span class="sd">    * `name`: the name of the tensor (default: `&quot;layer&quot;`)</span>
<span class="sd">    * `bias`: determines where to use a bias **b** or not (default: `True`)</span>
<span class="sd">    * `bias_name`: the name of the `w` tensor of type `tf.Variable` (default: `None`)</span>
<span class="sd">    ** Examples **</span>
<span class="sd">    # The following example shows you how to connect two tensors (rather builders) of different shapes to a single `softmax` layer of shape [None, 3]</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        a = tf.placeholder(tf.float32, shape=[None, 8]).builder()</span>
<span class="sd">        b = tf.placeholder(tf.float32, shape=[None, 5]).builder()</span>
<span class="sd">        h = (</span>
<span class="sd">            tb.branches([a, b])</span>
<span class="sd">            .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">        )</span>
<span class="sd">    The next example show you how you can use this to pass the input layer directly through one branch, and &quot;analyze&quot; it with a `tanh layer` filter through the other, both of these are connect to a single `softmax` output layer</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        h = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .branch(lambda x:</span>
<span class="sd">            [</span>
<span class="sd">                x</span>
<span class="sd">            ,</span>
<span class="sd">                x.connect_layer(10, fn=tf.nn.tanh)</span>
<span class="sd">            ])</span>
<span class="sd">            .connect_layer(3, fn=tf.nn.softmax)</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">builders</span> <span class="o">=</span> <span class="p">[</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_weights</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>
    <span class="n">builder</span> <span class="o">=</span> <span class="n">_add_builders</span><span class="p">(</span><span class="n">builders</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">connect_bias</span><span class="p">(</span><span class="n">bias_name</span><span class="o">=</span><span class="n">bias_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fn</span><span class="p">:</span>
        <span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builder</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.BuilderTree.copy">
    <p>def <span class="ident">copy</span>(</p><p>self)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree.copy', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree.copy" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">branches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">[:]</span>
    <span class="k">return</span> <span class="n">BuilderTree</span><span class="p">(</span><span class="n">branches</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="tensorbuilder.builders.BuilderTree.tensors">
    <p>def <span class="ident">tensors</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Same as <a href="#tensorbuilder.builders.BuilderTree.builders"><code>builders</code></a> but extracts the tensor from each <a href="#tensorbuilder.builders.Builder"><code>Builder</code></a>.</p>
<p><strong>Return</strong></p>
<ul>
<li><code>list( tf.Tensor )</code></li>
</ul>
<p><strong> Example </strong></p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="p">[</span><span class="n">h_tensor</span><span class="p">,</span> <span class="n">trainer_tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x</span><span class="o">.</span><span class="n">builder</span><span class="p">()</span>
    <span class="o">.</span><span class="n">connect_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">branch</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
    <span class="p">,</span>
        <span class="n">z</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="o">.</span><span class="n">tensors</span><span class="p">()</span>
<span class="p">)</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builders.BuilderTree.tensors', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builders.BuilderTree.tensors" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as `tensorbuilder.builders.BuilderTree.builders` but extracts the tensor from each `tensorbuilder.builders.Builder`.</span>
<span class="sd">    **Return**</span>
<span class="sd">    * `list( tf.Tensor )`</span>
<span class="sd">    ** Example **</span>
<span class="sd">        import tensorflow as tf</span>
<span class="sd">        import tensorbuilder as tb</span>
<span class="sd">        x = tf.placeholder(tf.float32, shape=[None, 5])</span>
<span class="sd">        y = tf.placeholder(tf.float32, shape=[None, 1])</span>
<span class="sd">        [h_tensor, trainer_tensor] = (</span>
<span class="sd">            x.builder()</span>
<span class="sd">            .connect_layer(1)</span>
<span class="sd">            .branch(lambda z:</span>
<span class="sd">            [</span>
<span class="sd">                z.map(tf.nn.sigmoid)</span>
<span class="sd">            ,</span>
<span class="sd">                z.map(tf.nn.sigmoid_cross_entropy_with_logits, y)</span>
<span class="sd">                .map(tf.train.AdamOptimizer(0.01).minimize)</span>
<span class="sd">            ])</span>
<span class="sd">            .tensors()</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">builder</span><span class="o">.</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">builder</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leafs</span><span class="p">()</span> <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
